{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919e393a-1aee-4211-a1fb-c6ef23f2f26d",
   "metadata": {},
   "source": [
    "# NLP - Homework 2\n",
    "### Miguel Bonilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36bd8d-65fb-4a5b-a90e-8b29f3bf7cf5",
   "metadata": {},
   "source": [
    "1. In Python, create a method for scoring the vocabulary size of a text, and normalize the score \n",
    "from 0 to 1. It does not matter what method you use for normalization as long as you explain it \n",
    "in a short paragraph. \n",
    "Some relevant resources that you can leverage:\n",
    "https://docs.tibco.com/pub/spotfire/6.5.0/doc/html/norm/norm_scale_between_0_and_1.htm\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "2. After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word \n",
    "vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n",
    "3. Now create a “text difficulty score” by combining the lexical diversity score from homework 1, \n",
    "and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. \n",
    "Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78955121-1c08-4f0c-8785-ca6a85b13df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a172625-41cf-4109-b4fe-85ae4ade9c67",
   "metadata": {},
   "source": [
    "- First https://www.gutenberg.org/cache/epub/14640/pg14640.txt\n",
    "- Second https://www.gutenberg.org/cache/epub/14668/pg14668.txt\n",
    "- Third https://www.gutenberg.org/cache/epub/14766/pg14766.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a26a18-2f80-48ba-bcc7-2fe611e8ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url1 = \"https://www.gutenberg.org/cache/epub/14640/pg14640.txt\"\n",
    "url2 = \"https://www.gutenberg.org/cache/epub/14668/pg14668.txt\"\n",
    "url3 = \"https://www.gutenberg.org/cache/epub/14766/pg14766.txt\"\n",
    "\n",
    "#import books\n",
    "text1 = request.urlopen(url1).read().decode('utf8')\n",
    "text2 = request.urlopen(url2).read().decode('utf8')\n",
    "text3 = request.urlopen(url3).read().decode('utf8')\n",
    "\n",
    "# tokenize books\n",
    "text1 = word_tokenize(text1)\n",
    "text2 = word_tokenize(text2)\n",
    "text3 = word_tokenize(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f10c036-6ce8-4c2a-8282-8fa3bc02b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning of text 1 is:  1125\n",
      "end of text 1 is:  9335\n",
      "beginning of text 2 is:  1500\n",
      "end of text 2 is:  22004\n",
      "beginning of text 3 is:  2116\n",
      "end of text 3 is:  34675\n"
     ]
    }
   ],
   "source": [
    "## text 1 find beginning and end\n",
    "print('beginning of text 1 is: ',text1.index('LESSON')) #beginning\n",
    "print('end of text 1 is: ',text1.index('PHONIC')-1) #end\n",
    "## text 2 find beginning and end\n",
    "print('beginning of text 2 is: ',text2.index('LESSON')) #beginning\n",
    "print('end of text 2 is: ',text2.index('Tennyson')) #end\n",
    "## text 3 find beginning and end\n",
    "print('beginning of text 3 is: ',text3.index('SHEPHERD')-4) #beginning\n",
    "print('end of text 3 is: ',text3.index('Follen')) #end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f359e75a-212b-4b93-8426-7d4499bfcffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text1[1125:9335]\n",
    "text2 = text2[1500:22004]\n",
    "text3 = text3[2116:34675]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51fcec-749d-42bd-8aa9-20ee399dd4fb",
   "metadata": {},
   "source": [
    "## Normalized Vocabulary Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4039cc3-7e58-469d-afff-2c3c69d236a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text1,text2,text3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a51817bd-5200-4df7-9044-08aaeb2ac1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "sizes = []\n",
    "def vocab_score(text_list):\n",
    "    for i in range(len(text_list)):\n",
    "        vocab_size = len(set(text_list[i]))\n",
    "        sizes.append(vocab_size)\n",
    "    for i in range(len(sizes)):\n",
    "        norm = (sizes[i]-np.asarray(sizes).min())/(np.asarray(sizes).max()-np.asarray(sizes).min())\n",
    "        scores.append(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a20c49b2-b0cc-438b-9831-98b85734437c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.7011711371363808, 1.0]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_score(texts)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "58ce2819-be44-46ba-a81b-16b767b8726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ffbd3813-5366-406c-bcd0-a7a4cf56f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()\n",
    "data = [[-1],[6],[10],[18]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1774e95c-ee84-46b6-a123-fd0d973abb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(sizes).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8e580f29-afce-4484-9ea4-8077ecbf8374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.70117114],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593fe060-9533-4119-958c-e1e64db5afc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
