{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919e393a-1aee-4211-a1fb-c6ef23f2f26d",
   "metadata": {},
   "source": [
    "# NLP - Homework 2\n",
    "### Miguel Bonilla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f4902-0a44-4fae-99c2-e064f9b3e21b",
   "metadata": {},
   "source": [
    "- [Normalized Vocabulary Scoring](#Normalized-Vocabulary-Scoring)\n",
    "- [Long Word Vocabulary Scoring](#Long-Word-Vocabulary-Scoring)\n",
    "- [Text Difficulty Scoring](#Text-Difficulty-Scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa36bd8d-65fb-4a5b-a90e-8b29f3bf7cf5",
   "metadata": {},
   "source": [
    "1. In Python, create a method for scoring the vocabulary size of a text, and normalize the score \n",
    "from 0 to 1. It does not matter what method you use for normalization as long as you explain it \n",
    "in a short paragraph. \n",
    "Some relevant resources that you can leverage:\n",
    "https://docs.tibco.com/pub/spotfire/6.5.0/doc/html/norm/norm_scale_between_0_and_1.htm\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "2. After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word \n",
    "vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.\n",
    "3. Now create a “text difficulty score” by combining the lexical diversity score from homework 1, \n",
    "and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. \n",
    "Explain what you see when this score is applied to same graded texts you used in homework 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78955121-1c08-4f0c-8785-ca6a85b13df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a172625-41cf-4109-b4fe-85ae4ade9c67",
   "metadata": {},
   "source": [
    "- First https://www.gutenberg.org/cache/epub/14640/pg14640.txt\n",
    "- Second https://www.gutenberg.org/cache/epub/14668/pg14668.txt\n",
    "- Third https://www.gutenberg.org/cache/epub/14766/pg14766.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a26a18-2f80-48ba-bcc7-2fe611e8ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url1 = \"https://www.gutenberg.org/cache/epub/14640/pg14640.txt\"\n",
    "url2 = \"https://www.gutenberg.org/cache/epub/14668/pg14668.txt\"\n",
    "url3 = \"https://www.gutenberg.org/cache/epub/14766/pg14766.txt\"\n",
    "\n",
    "#import books\n",
    "text1 = request.urlopen(url1).read().decode('utf8')\n",
    "text2 = request.urlopen(url2).read().decode('utf8')\n",
    "text3 = request.urlopen(url3).read().decode('utf8')\n",
    "\n",
    "# tokenize books\n",
    "text1 = word_tokenize(text1)\n",
    "text2 = word_tokenize(text2)\n",
    "text3 = word_tokenize(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f10c036-6ce8-4c2a-8282-8fa3bc02b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning of text 1 is:  1125\n",
      "end of text 1 is:  9335\n",
      "beginning of text 2 is:  1500\n",
      "end of text 2 is:  22004\n",
      "beginning of text 3 is:  2116\n",
      "end of text 3 is:  34675\n"
     ]
    }
   ],
   "source": [
    "## text 1 find beginning and end\n",
    "print('beginning of text 1 is: ',text1.index('LESSON')) #beginning\n",
    "print('end of text 1 is: ',text1.index('PHONIC')-1) #end\n",
    "## text 2 find beginning and end\n",
    "print('beginning of text 2 is: ',text2.index('LESSON')) #beginning\n",
    "print('end of text 2 is: ',text2.index('Tennyson')) #end\n",
    "## text 3 find beginning and end\n",
    "print('beginning of text 3 is: ',text3.index('SHEPHERD')-4) #beginning\n",
    "print('end of text 3 is: ',text3.index('Follen')) #end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f359e75a-212b-4b93-8426-7d4499bfcffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = text1[1125:9335]\n",
    "text2 = text2[1500:22004]\n",
    "text3 = text3[2116:34675]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10c70b9-937a-4bb2-935e-6d4010a74af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove word duplicates due to casing and non-alpha words\n",
    "## note that performing this step removes hyphenated words, which was deemed acceptable for this project since hyphenated words are combinations of words\n",
    "text1 = [w.lower() for w in text1 if w.isalpha()]\n",
    "text2 = [w.lower() for w in text2 if w.isalpha()]\n",
    "text3 = [w.lower() for w in text3 if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51fcec-749d-42bd-8aa9-20ee399dd4fb",
   "metadata": {},
   "source": [
    "## Normalized Vocabulary Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4039cc3-7e58-469d-afff-2c3c69d236a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text1,text2,text3] #list all 3 documents    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ebfce11-8db7-446b-866d-605d578539a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that iterates through document word counts and normalizes the counts into a score on a 0 to 1 scale\n",
    "def normalize_func(sizes_list,scores_list):\n",
    "    for i in range(len(sizes_list)):\n",
    "        norm = round(\n",
    "            (sizes_list[i][1]-np.asarray(list(zip(*sizes_list))[1]).min())/(np.asarray(list(zip(*sizes_list))[1]).max()-np.asarray(list(zip(*sizes_list))[1]).min()),\n",
    "            5)\n",
    "        scores_list.append((\"text{}\".format(i+1),norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51817bd-5200-4df7-9044-08aaeb2ac1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that iterates through the documents calculating the vocab size and then calls the previously defined norm_func to calculate the corresponding normalized scores.\n",
    "# it requires a text list and a blank scores_list which it passes to the normalize_func\n",
    "def vocab_score(text_list,scores_list):\n",
    "    for i in range(len(text_list)):\n",
    "        vocab_size = len(set(text_list[i]))\n",
    "        norm_sizes.append((\"text{}\".format(i+1),vocab_size))\n",
    "    normalize_func(norm_sizes,scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20c49b2-b0cc-438b-9831-98b85734437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text1', 882), ('text2', 2128), ('text3', 3095)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('text1', 0.0), ('text2', 0.56304), ('text3', 1.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_scores = []\n",
    "norm_sizes = []\n",
    "\n",
    "vocab_score(texts,norm_scores)\n",
    "print(norm_sizes)\n",
    "norm_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e15f5-5949-4801-ba17-9a4b8b20802c",
   "metadata": {},
   "source": [
    "The function vocab_score first calculates the unique word count for all 3 texts, then a normalized vocabulary score is calculated by taking a document's word count and providing a relative (normalized) measure of how it compares to the rest of the documents by subtracting the minimum word count and dividing it by the word count range between all documents. This provides a comparable score where all documents will have a measure between 0 and 1, with 0 and 1 representing the scores for the documents with the lowest and highest word count, accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48adc2a0-0e4d-426a-b2cb-8781dba537cb",
   "metadata": {},
   "source": [
    "## Long Word Vocabulary Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41f31b-df1b-4e08-9ee6-1102f4ae953e",
   "metadata": {},
   "source": [
    "Since word length is a relative concept, and considering the documents are for first, second, and third year learners, the following will use a \"long-word\" cutoff of 9 characters. Meaning, words with 9 or higher number of characters will be counted as long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83453724-d08e-4118-9c7d-09284aaa2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for calculating long word based on a long-word cutoff of 9 characters and above\n",
    "#it then calls the previously defined normalize_func to get the normalized scores in a range between 0 and 1\n",
    "def long_word_score(text_list,scores_list):\n",
    "    for i in range(len(text_list)):\n",
    "        long_word_size = len([w for w in set(text_list[i]) if len(w) > 8])\n",
    "        long_sizes.append((\"text{}\".format(1+i),long_word_size))\n",
    "    normalize_func(long_sizes,long_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a302a3cf-1c21-4b31-994e-7a67fe2df083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('text1', 13), ('text2', 92), ('text3', 253)]\n",
      "[('text1', 0.0), ('text2', 0.32917), ('text3', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "long_scores = []\n",
    "long_sizes = []\n",
    "\n",
    "long_word_score(texts,long_scores)\n",
    "print(long_sizes)\n",
    "print(long_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76a496-0778-4e6a-b939-45e0cbc9c0f4",
   "metadata": {},
   "source": [
    "The general approach here is similar to the approach for Normalized Vocabulary Score above. Normalization is achieved by using the same normalize function defined previously, which is integrated into the long word score function. Long words, defined here as having 9 or more characters in length, are identified, and the number of unique appearances counted. Then, the counts are normalized into a score that is relative to the long word count of all the documents in question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da071c-1ace-4d16-8879-71dc99ba8d12",
   "metadata": {},
   "source": [
    "## Text Difficulty Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b2309-db82-4e4e-933b-617b2f71438b",
   "metadata": {},
   "source": [
    "Calculate a text difficulty score by doing an equal weights average of both the normalized vocabulary score and the normalized long word score. Since the minimum and maximum scores for each were 0 and 1, respectively, the averaged out combined score will also be in the scale between 0 and 1 (since 0/2=0 and 2/2=1)\n",
    "\n",
    "#### needs to add from hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a587fe9d-4439-4fe3-9021-cc36f2e11e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_scores=[]\n",
    "for i in range(len(long_scores)):\n",
    "    diff_scores.append(('text{}'.format(i+1),(long_scores[i][1]+norm_scores[i][1])/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc4e195c-38f2-4950-916f-b16a1963d056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text1', 0.0), ('text2', 0.446105), ('text3', 1.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
